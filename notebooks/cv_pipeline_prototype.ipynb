{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# CV Pipeline Prototype\n",
    "**YOLOv8 → ByteTrack → MediaPipe Pose → Anomaly Detection**\n",
    "\n",
    "This notebook prototypes the full computer-vision pipeline before porting it to `cv_backend/core/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": ["## 1. Setup & Installs"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "installs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run once per Colab session\n",
    "!pip install ultralytics mediapipe lap --quiet\n",
    "\n",
    "# ByteTrack — install from source\n",
    "!git clone https://github.com/ifzhang/ByteTrack /content/ByteTrack --quiet\n",
    "%cd /content/ByteTrack\n",
    "!pip install -e . --quiet\n",
    "%cd /content\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import mediapipe as mp\n",
    "from IPython.display import display, Image as IPImage\n",
    "print('Setup complete. GPU:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": ["## 2. Upload Test Video"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upload",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "# Option A: upload from local machine\n",
    "# uploaded = files.upload()\n",
    "# VIDEO_PATH = list(uploaded.keys())[0]\n",
    "\n",
    "# Option B: mount Drive and point to a file\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# VIDEO_PATH = '/content/drive/MyDrive/test_video.mp4'\n",
    "\n",
    "# Option C: download a sample (Creative Commons fall detection video)\n",
    "# TODO: replace with your own test video URL\n",
    "VIDEO_PATH = '/content/test_video.mp4'\n",
    "print(f'Using video: {VIDEO_PATH}, exists: {os.path.exists(VIDEO_PATH)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": ["## 3. YOLOv8 Person Detection"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yolo",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8n.pt')  # auto-downloads on first run\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "ret, frame = cap.read()\n",
    "cap.release()\n",
    "\n",
    "if not ret:\n",
    "    print('Could not read video — check VIDEO_PATH')\n",
    "else:\n",
    "    results = model(frame, verbose=False)[0]\n",
    "    annotated = results.plot()\n",
    "    _, buf = cv2.imencode('.jpg', annotated)\n",
    "    display(IPImage(data=buf.tobytes()))\n",
    "    person_boxes = [b for b in results.boxes if int(b.cls[0]) == 0 and float(b.conf[0]) > 0.45]\n",
    "    print(f'Detected {len(person_boxes)} person(s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {},
   "source": ["## 4. ByteTrack Integration"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bytetrack",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolox.tracker.byte_tracker import BYTETracker\n",
    "\n",
    "class Args:\n",
    "    track_thresh = 0.45\n",
    "    track_buffer = 30\n",
    "    match_thresh = 0.8\n",
    "    mot20 = False\n",
    "\n",
    "tracker = BYTETracker(Args(), frame_rate=30)\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "frames_processed = 0\n",
    "\n",
    "while frames_processed < 60:  # process first 60 frames as demo\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    h, w = frame.shape[:2]\n",
    "    results = model(frame, verbose=False)[0]\n",
    "    dets = np.array(\n",
    "        [[*b.xyxy[0].tolist(), float(b.conf[0])] for b in results.boxes if int(b.cls[0]) == 0],\n",
    "        dtype=np.float32\n",
    "    ) if len(results.boxes) else np.empty((0, 5), dtype=np.float32)\n",
    "\n",
    "    online_targets = tracker.update(dets, [h, w], [h, w])\n",
    "\n",
    "    for t in online_targets:\n",
    "        x1, y1, bw, bh = t.tlwh\n",
    "        cv2.rectangle(frame, (int(x1), int(y1)), (int(x1+bw), int(y1+bh)), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f'ID:{t.track_id}', (int(x1), int(y1)-5),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "    frames_processed += 1\n",
    "\n",
    "cap.release()\n",
    "_, buf = cv2.imencode('.jpg', frame)\n",
    "display(IPImage(data=buf.tobytes()))\n",
    "print(f'Processed {frames_processed} frames, {len(online_targets)} tracked persons in last frame')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5",
   "metadata": {},
   "source": ["## 5. MediaPipe Pose on Tracked Persons"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediapipe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "pose = mp_pose.Pose(static_image_mode=True, model_complexity=1, min_detection_confidence=0.5)\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "ret, frame = cap.read()\n",
    "cap.release()\n",
    "\n",
    "results_yolo = model(frame, verbose=False)[0]\n",
    "person_boxes = [b.xyxy[0].tolist() for b in results_yolo.boxes if int(b.cls[0]) == 0 and float(b.conf[0]) > 0.45]\n",
    "\n",
    "for bbox in person_boxes:\n",
    "    x1, y1, x2, y2 = [int(v) for v in bbox]\n",
    "    crop = frame[max(0,y1):y2, max(0,x1):x2]\n",
    "    crop_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
    "    pose_result = pose.process(crop_rgb)\n",
    "\n",
    "    if pose_result.pose_landmarks:\n",
    "        mp_draw.draw_landmarks(crop, pose_result.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "        frame[max(0,y1):y2, max(0,x1):x2] = crop\n",
    "\n",
    "_, buf = cv2.imencode('.jpg', frame)\n",
    "display(IPImage(data=buf.tobytes()))\n",
    "print(f'Pose detected for {len(person_boxes)} person(s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6",
   "metadata": {},
   "source": ["## 6. Anomaly Logic — Fall & Collapse Detection"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anomaly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fine-tune these thresholds once you have real fall videos\n",
    "FALL_ASPECT_RATIO = 1.4   # width/height > this → possible fall\n",
    "ERRATIC_VAR_THRESHOLD = 800.0\n",
    "\n",
    "from collections import defaultdict, deque\n",
    "import time\n",
    "\n",
    "track_history = defaultdict(lambda: deque(maxlen=30))\n",
    "\n",
    "def check_fall(bbox):\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    w, h = x2 - x1, y2 - y1\n",
    "    if h <= 0:\n",
    "        return False, 0.0\n",
    "    ratio = w / h\n",
    "    return ratio > FALL_ASPECT_RATIO, round(min(ratio / 3.0, 1.0), 3)\n",
    "\n",
    "def check_erratic(history):\n",
    "    if len(history) < 10:\n",
    "        return False, 0.0\n",
    "    recent = list(history)[-10:]\n",
    "    variance = float(np.var([p[0] for p in recent]) + np.var([p[1] for p in recent]))\n",
    "    return variance > ERRATIC_VAR_THRESHOLD, round(min(variance / (ERRATIC_VAR_THRESHOLD * 3), 1.0), 3)\n",
    "\n",
    "# Quick test\n",
    "test_bbox_fall = [10, 100, 200, 130]  # very wide — should flag as fall\n",
    "test_bbox_normal = [10, 50, 80, 200]  # tall — normal standing\n",
    "print('Fall test (wide bbox):', check_fall(test_bbox_fall))\n",
    "print('Normal test (tall bbox):', check_fall(test_bbox_normal))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7",
   "metadata": {},
   "source": ["## 7. End-to-End Pipeline — Annotated Output"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fine-tune INFERENCE_STRIDE for your target FPS\n",
    "INFERENCE_STRIDE = 2\n",
    "OUTPUT_PATH = '/content/annotated_output.mp4'\n",
    "\n",
    "tracker2 = BYTETracker(Args(), frame_rate=30)\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) or 30\n",
    "fw = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "fh = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "writer = cv2.VideoWriter(OUTPUT_PATH, cv2.VideoWriter_fourcc(*'mp4v'), fps, (fw, fh))\n",
    "\n",
    "frame_id = 0\n",
    "anomaly_log = []\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame_id += 1\n",
    "\n",
    "    if frame_id % INFERENCE_STRIDE != 0:\n",
    "        writer.write(frame)\n",
    "        continue\n",
    "\n",
    "    results_y = model(frame, verbose=False)[0]\n",
    "    dets = np.array(\n",
    "        [[*b.xyxy[0].tolist(), float(b.conf[0])] for b in results_y.boxes if int(b.cls[0]) == 0],\n",
    "        dtype=np.float32\n",
    "    ) if len(results_y.boxes) else np.empty((0, 5), dtype=np.float32)\n",
    "\n",
    "    tracks = tracker2.update(dets, [fh, fw], [fh, fw])\n",
    "\n",
    "    for t in tracks:\n",
    "        x1, y1, bw, bh = t.tlwh\n",
    "        bbox = [x1, y1, x1+bw, y1+bh]\n",
    "        cx, cy = x1 + bw/2, y1 + bh/2\n",
    "        track_history[t.track_id].append((cx, cy))\n",
    "\n",
    "        is_fall, fall_conf = check_fall(bbox)\n",
    "        is_erratic, erratic_conf = check_erratic(track_history[t.track_id])\n",
    "\n",
    "        color = (0, 255, 0)\n",
    "        label = f'ID:{t.track_id}'\n",
    "        if is_fall:\n",
    "            color = (0, 0, 255)\n",
    "            label += ' FALL'\n",
    "            anomaly_log.append({'frame': frame_id, 'type': 'FALL', 'track_id': t.track_id, 'confidence': fall_conf})\n",
    "        elif is_erratic:\n",
    "            color = (0, 165, 255)\n",
    "            label += ' ERRATIC'\n",
    "            anomaly_log.append({'frame': frame_id, 'type': 'ERRATIC_MOTION', 'track_id': t.track_id, 'confidence': erratic_conf})\n",
    "\n",
    "        cv2.rectangle(frame, (int(x1), int(y1)), (int(x1+bw), int(y1+bh)), color, 2)\n",
    "        cv2.putText(frame, label, (int(x1), int(y1)-5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "    writer.write(frame)\n",
    "\n",
    "cap.release()\n",
    "writer.release()\n",
    "print(f'Done. Output: {OUTPUT_PATH}')\n",
    "print(f'Anomaly events detected: {len(anomaly_log)}')\n",
    "for ev in anomaly_log[:10]:\n",
    "    print(ev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-8",
   "metadata": {},
   "source": [
    "## 8. Export Colab → FastAPI\n",
    "\n",
    "| Notebook cell | Maps to FastAPI file |\n",
    "|---|---|\n",
    "| `model = YOLO(...)` + detection loop | `cv_backend/core/detector.py` → `AnomalyDetector` |\n",
    "| `BYTETracker` + update loop | `cv_backend/core/tracker.py` → `PersonTracker` |\n",
    "| `mp_pose.Pose` + crop inference | `cv_backend/core/pose.py` → `PoseAnalyzer` |\n",
    "| `check_fall`, `check_erratic` functions | `cv_backend/core/anomaly.py` → `AnomalyClassifier` |\n",
    "| Full while-loop pipeline | `cv_backend/routers/stream.py` → `_run_pipeline()` |\n",
    "\n",
    "When porting:\n",
    "1. Replace `print()` with proper logging.\n",
    "2. Wrap blocking calls in `asyncio.to_thread()`.\n",
    "3. Add full type hints to all methods."
   ]
  }
 ]
}
